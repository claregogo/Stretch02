---
title: "Data Science for Public Policy"
subtitle: "Stretch 02"
author: "Zehui Li, Mujin Li"
execute:
  warning: false
format:
  html:
    embed-resources: true
---

## 1. Set up

### load the packages

```{r}
library(mice) # for imputating missing value
library(dplyr)
library(here)
library(withr)
library(tidyverse)
library(lubridate)
library(tidymodels)
library(themis)
library(recipes)
library(parsnip)
library(ranger)
library(ggplot2)
library(vip)
library(patchwork)
library(readr)       # for importing data
```

### a. Data preparation and introduction
Predicting Childcare Costs in a County

The dataset we are utilizing is sourced from the [National Database of Childcare Prices (NDCP)](https://www.dol.gov/agencies/wb/topics/featured-childcare), which serves as a comprehensive federal repository of childcare pricing information at the county level. This dataset offers detailed insights into childcare costs, categorized by childcare provider type, children's age groups, and various county characteristics. Covering the period from 2008 to 2018, it encompasses a wide range of socio-economic variables pertinent to counties across the United States. It's worth noting that the initial data cleaning and acquisition processes were conducted as part of the Tidy Tuesday project, and you can find further details about the dataset [here](https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-05-09/readme.md).

The objective of this regression application is to build a predictive model that can estimate the median weekly cost of Center-Based Care for school-age children in a county based on various socio-economic and demographic features of that county. The outcome variable for this regression application is the median weekly cost of Center-Based Care for school-age children (`mcsa`). Center-Based Care refers to childcare provided in a daycare center or facility. our predictor variables include:`unr_16`: Unemployment rate of the population aged 16 years old or older.`pr_f`: Poverty rate for families.`mhi_2018`: Median household income expressed in 2018 dollars.`total_pop`: Count of the total population etc.


```{r}
# All packages used in this script:
library(tidyverse)
library(here)
library(withr)

url <- "https://www.dol.gov/sites/dolgov/files/WB/media/nationaldatabaseofchildcareprices.xlsx"
temp_xlsx <- withr::local_tempfile(fileext = ".xlsx")
download.file(url, temp_xlsx, mode = "wb")

childcare_costs_raw <- readxl::read_xlsx(temp_xlsx) |>
  janitor::clean_names() |> 
  # There are 15 constant columns. Get rid of those.
  janitor::remove_constant(quiet = FALSE)

# The file is very large, but it contains a lot of duplicate data. Extract
# duplications into their own tables.
counties <- childcare_costs_raw |> 
  dplyr::distinct(county_fips_code, county_name, state_name, state_abbreviation)
childcare_costs <- childcare_costs_raw |> 
  dplyr::select(
    -county_name,
    -state_name,
    -state_abbreviation,
    # Original data also contained unadjusted + adjusted dollars, let's just
    # keep the 2018 adjustments.
    -mhi, -me, -fme, -mme,
    # A number of columns have fine-grained breakdowns by age, and then also
    # broader categories. Let's only keep the categories ("infant" vs 0-5
    # months, 6-11 monts, etc)
    -ends_with("bto5"), -ends_with("6to11"), -ends_with("12to17"), 
    -ends_with("18to23"), -ends_with("24to29"), -ends_with("30to35"),
    -ends_with("36to41"), -ends_with("42to47"), -ends_with("48to53"),
    -ends_with("54to_sa"),
    # Since we aren't worrying about the unaggregated columns, we can ignore the
    # flags indicating how those columns were aggregated into the combined
    # columns.
    -ends_with("_flag"),
    # Original data has both median and 75th percentile for a number of columns.
    # We'll simplify.
    -starts_with("x75"),
    # While important for wider research, we don't need to keep the (many)
    # variables describing whether certain data was imputed.
    -starts_with("i_")
  )
```

### b.Split the data into training and testing data

predictor: Center-Based Care for those who are school age based

```{r}
childcare_costs
```

```{r}
# select mcsa as my outcome
childcare_costs<- subset(childcare_costs, select = -c(mfccsa, mc_infant, mc_toddler, mc_preschool, mfcc_infant, mfcc_toddler, mfcc_preschool))
# missing value test
childcare_costs%>%
  group_by(county_fips_code)%>%
  summarise(missing_values = sum(is.na(mcsa)))
# use mean value to fill missing value
childcare_costs$h_under6_single_m <- ifelse(is.na(childcare_costs$h_under6_single_m), mean(childcare_costs$h_under6_single_m, na.rm = TRUE), childcare_costs$h_under6_single_m)
childcare_costs$h_6to17_single_m <- ifelse(is.na(childcare_costs$h_6to17_single_m), mean(childcare_costs$h_6to17_single_m, na.rm = TRUE), childcare_costs$h_6to17_single_m)
# pick out the rows includes the missing value
implement <- childcare_costs[is.na(childcare_costs$mcsa), ]
childcare_modeling <- childcare_costs[complete.cases(childcare_costs), ]
# examine the missing value
childcare_modeling %>%
  summarise_all(~ sum(is.na(.)))
glimpse(childcare_costs$mcsa)
```

```{r}
# select the column with large value
needtolog <- childcare_modeling %>%
  select(starts_with("h")) %>%
  names()
needtolog <- needtolog[needtolog != "hispanic"]
needtolog <- c(needtolog, "total_pop")
```

#### split data into train and test

```{r}
set.seed(701)
split <-initial_split(childcare_modeling,strata = c("county_fips_code"))
train<-training(split)
test<-testing(split)
```

### c.Explortary data analysis
```{r}
#Distribution about mcsa
ggplot(data = train, aes(x = mcsa)) +
  geom_histogram(fill = "blue", color = "black", bins = 30) +
  labs(title = "Distribution of MCSA",
       x = "MCSA",
       y = "Frequency")+
  theme_minimal()
# Scatter Plot of MCSA vs. Median Household Income
ggplot(data = train, aes(x = mhi_2018, y = mcsa)) +
  geom_point() +
  labs(title = "Scatter Plot of MCSA vs. Median Household Income",
       x = "Median Household Income (2018)",
       y = "MCSA")+
  theme_minimal()
# Bar chart for 'study_year'
ggplot(data = train, aes(x = factor(study_year))) +
  geom_bar(fill = "blue", color = "black",width = 0.5) +
  labs(title = "Distribution of Study Years",
       x = "Study Year",
       y = "Count")+
  theme_minimal()
```

### d.Error Metric

RMSE, which stands for Root Mean Square Error, is a commonly used metric in regression analysis to quantify the average magnitude of errors between predicted values and actual observed values. It measures the square root of the average of the squared differences between predicted and observed values. Larger errors, as indicated by RMSE, generally incur higher costs. "mcsa" relates to childcare costs, larger prediction errors might affect budget planning or subsidy allocation.There is no universal threshold for RMSE, and it should be determined according to the application's requirements and acceptable level of prediction accuracy. Generally, a lower RMSE values indicating better model performance. Since we are dealing with a regression problem, the concepts of false positives and false negatives are more relevant in classification tasks.
MSE: $$ MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$ 
RMSE: $$ RMSE = \sqrt{MSE} $$ 
MAE: $$ MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| $$

#### set resample
```{r}
set.seed(234)
folds<-vfold_cv(train, v = 10)
```

## 2&3 come up with models

### create recipe

```{r}
recipe<-recipe(mcsa~.,data = train)%>%
  step_dummy(all_nominal_predictors())%>%
  step_zv(all_predictors()) %>% 
  step_impute_knn(all_predictors())%>%
  step_scale(all_numeric_predictors())
bake(prep(recipe, training = train),new_data = train)
train%>%
  summarise_all(~ sum(is.na(.)))
```

### penalized lasso regression

```{r}
# use Cross-Validation
lasso_grid <- grid_regular(penalty(), levels = 1000)
lasso_mod <- linear_reg(
  penalty = tune(), 
  mixture = 1
) %>%
  set_engine("glmnet", path_values = lasso_grid$penalty)
#create the workflow
lasso_wf<-
  workflow()%>%
  add_model(lasso_mod)%>%
  add_recipe(recipe)
#train and tune the model 
lasso_res <- 
  lasso_wf %>% 
  tune_grid(folds,#use resample
            grid = lasso_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
# plot the visulization of penalty and rmse
lasso_rmse<-
  collect_metrics(lasso_res, summarize = FALSE) %>% 
  filter(.metric == "rmse") 

lasso_plot <-ggplot(lasso_rmse, aes(x = penalty, y = .estimate,color = id)) +
  geom_line() +
  geom_point() +
  scale_x_log10(labels = scales::label_number())+
  labs(title = "RMSE Across Lasso Resamples",
       x = "penalty",
       y = "RMSE") +
  theme_minimal()

lasso_plot 
```

### the last fit & visualize the variable importance scores for the top 20 features

```{r}
# the last model
lasso_best<-lasso_res%>%
  select_best(metric = "rmse")
lasso_best

best_lasso_mod <- 
  linear_reg(penalty = lasso_best$penalty, mixture = 1) %>%
  set_engine("glmnet",importance = "impurity")

# the last workflow
best_lasso_wf <- 
  lasso_wf %>% 
  update_model(best_lasso_mod)

# the last fit
set.seed(701)
best_lasso_fit <- best_lasso_wf %>% 
  last_fit(split)

best_lasso_fit

best_lasso_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 20)
```

### random forest

```{r}
#use the parallel package to query the number of cores on my computer 
cores <- parallel::detectCores()
cores
#build the model
rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger",num.threads = 10) %>% 
  set_mode("regression")
#create the workflow
rf_wf <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(recipe)
# train and tune the model
set.seed(345)
rf_res <- 
  rf_wf %>% 
  tune_grid(folds,
            grid = 5,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
# find the best mtry&min_n value
rf_res %>% 
  show_best(metric = "rmse")

# plot the visulization of mtry and rmse
rf_rmse<-
  collect_metrics(rf_res, summarize = FALSE) %>% 
  filter(.metric == "rmse") 

rf_plot <-ggplot(rf_rmse, aes(x = mtry, y = .estimate,color = id)) +
  geom_line() +
  geom_point() +
  labs(title = "RMSE Across Random Forest Resamples",
       x = "mtry",
       y = "RMSE") +
  theme_minimal()

rf_plot 

```

### Estimate the out-of-sample error rate

```{r}
#select my best random forest model
rf_best<-rf_res%>%
  select_best(metric = "rmse")
rf_best
# use best model make predictions with test dataset
rf_best_test <- finalize_workflow(
  rf_wf,
  parameters = rf_best
)
rf_best_test_fit<-rf_best_test%>%
  fit(data = train) # use original train dataset
test_prediction <- rf_best_test_fit%>%
  predict(new_data = test) #use test dataset

rmse_test <- bind_cols(test %>% select(mcsa),  
                       test_prediction %>% select(.pred)
                       )%>% #combine the true value from test dataset and the predicted value
  rmse(truth = mcsa, estimate = .pred)
rmse_test
```

### the last fit & visualize the variable importance scores for the top 20 features

```{r}

# the last model
best_rf_mod <- 
  rand_forest(mtry = 32, min_n = 6, trees = 1000) %>% 
  set_engine("ranger",importance = "impurity") %>% 
  set_mode("regression")

# the last workflow
best_rf_wf <- 
  rf_wf %>% 
  update_model(best_rf_mod)

# the last fit
set.seed(345)
best_rf_fit <- best_rf_wf %>% 
  last_fit(split)

best_rf_fit

best_rf_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 20)
```

```{r}
new_recipe<-recipe(mcsa~total_pop+households+h_6to17_single_m+mhi_2018+h_under6_single_m,data = train)%>%
  step_dummy(all_nominal_predictors())%>%
  step_zv(all_predictors()) %>% 
  step_impute_knn(all_predictors())%>%
  step_scale(all_numeric_predictors())
bake(prep(recipe, training = train),new_data = train)
train%>%
  summarise_all(~ sum(is.na(.)))
#build the model
rf_new_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger",num.threads = 10) %>% 
  set_mode("regression")
#create the workflow
rf_new_wf <- 
  workflow() %>% 
  add_model(rf_new_mod) %>% 
  add_recipe(recipe)
# train and tune the model
set.seed(998)
rf_new_res <- 
  rf_new_wf %>% 
  tune_grid(folds,
            grid = 5,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
# find the best mtry&min_n value
rf_new_res %>% 
  show_best(metric = "rmse")

# plot the visulization of mtry and rmse
rf_new_rmse<-
  collect_metrics(rf_new_res, summarize = FALSE) %>% 
  filter(.metric == "rmse") 

rf_new_plot <-ggplot(rf_new_rmse, aes(x = mtry, y = .estimate,color = id)) +
  geom_line() +
  geom_point() +
  labs(title = "RMSE Across Random Forest Resamples",
       x = "mtry",
       y = "RMSE") +
  theme_minimal()

rf_new_plot 
```

## 4.Interpretation

We employed Lasso Regression and Random Forest Regression, with RMSE as our evaluation metric. After evaluating the model performances, we can draw the following conclusions:
The Lasso Regression model achieved a lower RMSE of approximately 20, but the Random Forest Regression model obtained an even lower RMSE of around 10 compared to Lasso. This indicates that the Random Forest model performed better, exhibiting better predictive accuracy in terms of childcare cost estimation.
While both models have their merits and can be valuable in practical applications, we observed a notable difference when examining the importance scores of individual features for each regression model. In particular, the Random Forest model identified the county code as the most important feature, which suggests a potential risk of overfitting.
To address this concern, we decided to utilize the Lasso Regression model to select the top 5 features based on their importance.The top 5 features in lasso are`total_pop` (Total Population)`households` (Number of Households) `h_6to17_single_m` (Median Household Income for Households with Children Aged 6 to 17)`rhi_2018` (Racial and Ethnic Diversity Index in 2018) `h_under6_single_m` (Median Household Income for Households with Children Under 6 Years Old), See importance score picture above. We will then incorporate these selected features into the Random Forest model. This aims to combine the feature selection capability of Lasso with the predictive power of Random Forest, potentially leading to a more accurate prediction.
To imporve our model and reduce RMSE, several points can be considered. Firstly, transitioning from county-level data to state-level data may lead to improved performance, though it entails significant effort and further learning. Secondly, despite our optimization efforts, the final RMSE reduction was not substantial. Lastly, we limited our feature selection to only five variables; increasing this count to ten could potentially yield better results. These steps signify areas where additional exploration and refinement are needed to enhance our model's predictive accuracy.


