---
title: "Data Science for Public Policy"
subtitle: "Stretch 02"
author: "Zehui Li, Mujin Li"
execute:
  warning: false
format:
  html:
    embed-resources: true
---

## 1. set up

### library

```{r}
library(mice) # for imputating missing value
library(dplyr)
library(here)
library(withr)
library(tidyverse)
library(lubridate)
library(tidymodels)
library(themis)
library(recipes)
library(parsnip)
library(ranger)
library(ggplot2)
library(vip)
library(patchwork)
library(readr)       # for importing data
```

### data setup

[data from tidytuesday](https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-05-09/readme.md)

```{r}
# All packages used in this script:
library(tidyverse)
library(here)
library(withr)

url <- "https://www.dol.gov/sites/dolgov/files/WB/media/nationaldatabaseofchildcareprices.xlsx"
temp_xlsx <- withr::local_tempfile(fileext = ".xlsx")
download.file(url, temp_xlsx, mode = "wb")

childcare_costs_raw <- readxl::read_xlsx(temp_xlsx) |>
  janitor::clean_names() |> 
  # There are 15 constant columns. Get rid of those.
  janitor::remove_constant(quiet = FALSE)

# The file is very large, but it contains a lot of duplicate data. Extract
# duplications into their own tables.
counties <- childcare_costs_raw |> 
  dplyr::distinct(county_fips_code, county_name, state_name, state_abbreviation)
childcare_costs <- childcare_costs_raw |> 
  dplyr::select(
    -county_name,
    -state_name,
    -state_abbreviation,
    # Original data also contained unadjusted + adjusted dollars, let's just
    # keep the 2018 adjustments.
    -mhi, -me, -fme, -mme,
    # A number of columns have fine-grained breakdowns by age, and then also
    # broader categories. Let's only keep the categories ("infant" vs 0-5
    # months, 6-11 monts, etc)
    -ends_with("bto5"), -ends_with("6to11"), -ends_with("12to17"), 
    -ends_with("18to23"), -ends_with("24to29"), -ends_with("30to35"),
    -ends_with("36to41"), -ends_with("42to47"), -ends_with("48to53"),
    -ends_with("54to_sa"),
    # Since we aren't worrying about the unaggregated columns, we can ignore the
    # flags indicating how those columns were aggregated into the combined
    # columns.
    -ends_with("_flag"),
    # Original data has both median and 75th percentile for a number of columns.
    # We'll simplify.
    -starts_with("x75"),
    # While important for wider research, we don't need to keep the (many)
    # variables describing whether certain data was imputed.
    -starts_with("i_")
  )
```

### Split the data into training and testing data

predictor: Center-Based Care for those who are school age based

```{r}
childcare_costs
```

```{r}
# select mcsa as my outcome
childcare_costs<- subset(childcare_costs, select = -c(mfccsa, mc_infant, mc_toddler, mc_preschool, mfcc_infant, mfcc_toddler, mfcc_preschool))
# missing value test
childcare_costs%>%
  group_by(county_fips_code)%>%
  summarise(missing_values = sum(is.na(mcsa)))
# use mean value to fill missing value
childcare_costs$h_under6_single_m <- ifelse(is.na(childcare_costs$h_under6_single_m), mean(childcare_costs$h_under6_single_m, na.rm = TRUE), childcare_costs$h_under6_single_m)
childcare_costs$h_6to17_single_m <- ifelse(is.na(childcare_costs$h_6to17_single_m), mean(childcare_costs$h_6to17_single_m, na.rm = TRUE), childcare_costs$h_6to17_single_m)
# pick out the rows includes the missing value
implement <- childcare_costs[is.na(childcare_costs$mcsa), ]
childcare_modeling <- childcare_costs[complete.cases(childcare_costs), ]
# examine the missing value
childcare_modeling %>%
  summarise_all(~ sum(is.na(.)))
glimpse(childcare_costs$mcsa)
```

```{r}
# select the column with large value
needtolog <- childcare_modeling %>%
  select(starts_with("h")) %>%
  names()
needtolog <- needtolog[needtolog != "hispanic"]
needtolog <- c(needtolog, "total_pop")
```

#### split data into train and test

```{r}
set.seed(701)
split <-initial_split(childcare_modeling,strata = c("county_fips_code"))
train<-training(split)
test<-testing(split)
```

#### set resample

```{r}
set.seed(234)
folds<-vfold_cv(train, v = 10)
```

## 2.come up with models

### create recipe

```{r}
recipe<-recipe(mcsa~.,data = train)%>%
  step_dummy(all_nominal_predictors())%>%
  step_zv(all_predictors()) %>% 
  step_impute_knn(all_predictors())%>%
  step_scale(all_numeric_predictors())
bake(prep(recipe, training = train),new_data = train)
train%>%
  summarise_all(~ sum(is.na(.)))
```

### penalized lasso regression

```{r}
# use Cross-Validation
lasso_grid <- grid_regular(penalty(), levels = 1000)
lasso_mod <- linear_reg(
  penalty = tune(), 
  mixture = 1
) %>%
  set_engine("glmnet", path_values = lasso_grid$penalty)
#create the workflow
lasso_wf<-
  workflow()%>%
  add_model(lasso_mod)%>%
  add_recipe(recipe)
#train and tune the model 
lasso_res <- 
  lasso_wf %>% 
  tune_grid(folds,#use resample
            grid = lasso_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
# plot the visulization of penalty and rmse
lasso_rmse<-
  collect_metrics(lasso_res, summarize = FALSE) %>% 
  filter(.metric == "rmse") 

lasso_plot <-ggplot(lasso_rmse, aes(x = penalty, y = .estimate,color = id)) +
  geom_line() +
  geom_point() +
  scale_x_log10(labels = scales::label_number())+
  labs(title = "RMSE Across Lasso Resamples",
       x = "penalty",
       y = "RMSE") +
  theme_minimal()

lasso_plot 
```

### random forest

```{r}
#use the parallel package to query the number of cores on my computer 
cores <- parallel::detectCores()
cores
#build the model
rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger",num.threads = 10) %>% 
  set_mode("regression")
#create the workflow
rf_wf <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(recipe)
# train and tune the model
set.seed(345)
rf_res <- 
  rf_wf %>% 
  tune_grid(folds,
            grid = 5,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
# find the best mtry&min_n value
rf_res %>% 
  show_best(metric = "rmse")

# plot the visulization of mtry and rmse
rf_rmse<-
  collect_metrics(rf_res, summarize = FALSE) %>% 
  filter(.metric == "rmse") 

rf_plot <-ggplot(rf_rmse, aes(x = mtry, y = .estimate,color = id)) +
  geom_line() +
  geom_point() +
  labs(title = "RMSE Across Random Forest Resamples",
       x = "mtry",
       y = "RMSE") +
  theme_minimal()

rf_plot 

```

### Estimate the out-of-sample error rate

```{r}
#select my best random forest model
rf_best<-rf_res%>%
  select_best(metric = "rmse")
rf_best
# use best model make predictions with test dataset
rf_best_test <- finalize_workflow(
  rf_wf,
  parameters = rf_best
)
rf_best_test_fit<-rf_best_test%>%
  fit(data = train) # use original train dataset
test_prediction <- rf_best_test_fit%>%
  predict(new_data = test) #use test dataset

rmse_test <- bind_cols(test %>% select(mcsa),  
                       test_prediction %>% select(.pred)
                       )%>% #combine the true value from test dataset and the predicted value
  rmse(truth = mcsa, estimate = .pred)
rmse_test
```

### the last fit&visualize the variable importance scores for the top 20 features

```{r}

# the last model
best_rf_mod <- 
  rand_forest(mtry = 32, min_n = 6, trees = 1000) %>% 
  set_engine("ranger",importance = "impurity") %>% 
  set_mode("regression")

# the last workflow
best_rf_wf <- 
  rf_wf %>% 
  update_model(best_rf_mod)

# the last fit
set.seed(345)
best_rf_fit <- best_rf_wf %>% 
  last_fit(split)

best_rf_fit

best_rf_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 20)
```
